{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pynvml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ1LuC5QUxvv",
        "outputId": "12ac1785-065c-4672-9b7d-227a5d9818a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (12.0.0)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml) (12.575.51)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlDeviceGetUtilizationRates, nvmlShutdown\n",
        "\n",
        "import threading"
      ],
      "metadata": {
        "id": "q-t5qbcBU2LV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "moyQElXcT5ui"
      },
      "outputs": [],
      "source": [
        "class GPUMonitor:\n",
        "    def __init__(self, monitoring_interval: float = 0.1, device_index: int = 0):\n",
        "        self.monitoring_interval = monitoring_interval\n",
        "        self.device_index = device_index\n",
        "        self._gpu_memory_usage = []\n",
        "        self._gpu_utilization = []\n",
        "        self._is_monitoring = False\n",
        "        self._thread = None\n",
        "\n",
        "    def _monitor(self):\n",
        "        nvmlInit()\n",
        "        handle = nvmlDeviceGetHandleByIndex(self.device_index)\n",
        "        while self._is_monitoring:\n",
        "            mem_info = nvmlDeviceGetMemoryInfo(handle)\n",
        "            util_info = nvmlDeviceGetUtilizationRates(handle)\n",
        "            self._gpu_memory_usage.append(mem_info.used / (1024 ** 2))  # in MB\n",
        "            self._gpu_utilization.append(util_info.gpu)\n",
        "            time.sleep(self.monitoring_interval)\n",
        "        nvmlShutdown()\n",
        "\n",
        "    def start(self):\n",
        "        self._is_monitoring = True\n",
        "        self._thread = threading.Thread(target=self._monitor)\n",
        "        self._thread.start()\n",
        "\n",
        "    def stop(self):\n",
        "        self._is_monitoring = False\n",
        "        self._thread.join()\n",
        "\n",
        "    def get_peak_usage(self):\n",
        "        return {\n",
        "            'peak_gpu_memory_mb': max(self._gpu_memory_usage, default=0),\n",
        "            'p90_gpu_utilization': np.percentile(self._gpu_utilization, 90) if self._gpu_utilization else 0\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark_language_model(model, tokenizer, prompts, temperature=0.7, max_new_tokens=100):\n",
        "    model.eval()\n",
        "    model.cuda()\n",
        "\n",
        "    gpu_monitor = GPUMonitor()\n",
        "    gpu_monitor.start()\n",
        "\n",
        "    ttft_list = []\n",
        "    tps_list = []\n",
        "    decode_tps_list = []\n",
        "\n",
        "    for prompt in prompts:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "        input_len = inputs.input_ids.shape[-1]\n",
        "\n",
        "        # Measure time to first token\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Jalankan generate dan ukur waktu hingga token pertama (berdasarkan waktu nyata)\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                **inputs,\n",
        "                do_sample=True,\n",
        "                temperature=temperature,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=False  # tidak perlu lagi\n",
        "            )\n",
        "\n",
        "        gen_time = time.time() - start_time\n",
        "        ttft = gen_time / max(output.sequences.shape[-1] - input_len, 1)\n",
        "\n",
        "\n",
        "        total_tokens = output.sequences.shape[-1]\n",
        "        output_tokens = total_tokens - input_len\n",
        "\n",
        "        ttft_list.append(ttft)\n",
        "        tps_list.append(total_tokens / gen_time)\n",
        "        decode_tps_list.append(output_tokens / gen_time)\n",
        "\n",
        "    gpu_monitor.stop()\n",
        "    gpu_stats = gpu_monitor.get_peak_usage()\n",
        "\n",
        "    return {\n",
        "        'p90_ttft_seconds': np.percentile(ttft_list, 90),\n",
        "        'p90_total_tps': np.percentile(tps_list, 90),\n",
        "        'p90_output_decode_tps': np.percentile(decode_tps_list, 90),\n",
        "        'max_gpu_memory_mb': gpu_stats['peak_gpu_memory_mb'],\n",
        "        'p90_gpu_utilization': gpu_stats['p90_gpu_utilization'],\n",
        "    }"
      ],
      "metadata": {
        "id": "iVuKj3wYU6py"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\").cuda()\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
        "\n",
        "# test_prompts = [\n",
        "#     \"Short prompt.\",\n",
        "#     \"This is a medium length prompt with some context and words.\",\n",
        "#     \"This is a very long prompt designed to simulate a large context. \" * 10\n",
        "# ]\n",
        "\n",
        "# results = benchmark_language_model(\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     prompts=test_prompts,\n",
        "#     temperature=0.7,\n",
        "#     max_new_tokens=100\n",
        "# )\n",
        "\n",
        "# print(f\"P90 TPS: {results['p90_total_tps']:.2f}\")\n",
        "# print(f\"P90 TTFT: {results['p90_ttft_seconds']:.4f} seconds\")\n",
        "# print(f\"P90 Output Decode TPS: {results['p90_output_decode_tps']:.2f}\")\n",
        "# print(f\"Max GPU Memory: {results['max_gpu_memory_mb']:.2f} MB\")\n",
        "# print(f\"P90 GPU Utilization: {results['p90_gpu_utilization']:.2f}%\")"
      ],
      "metadata": {
        "id": "nnwuYRUpULaC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import gc\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# import numpy as np\n",
        "\n",
        "# def compare_llms(model_names, prompts, temperature=0.7, max_new_tokens=100):\n",
        "#     all_results = {}\n",
        "\n",
        "#     for model_name in model_names:\n",
        "#         print(f\"\\nüîç Benchmarking model: {model_name}\")\n",
        "\n",
        "#         try:\n",
        "#             # Load model dan tokenizer\n",
        "#             model = AutoModelForCausalLM.from_pretrained(\n",
        "#                 model_name,\n",
        "#                 torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "#             ).cuda()\n",
        "#             tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "#             # Setup tokenizer defaults if missing\n",
        "#             if tokenizer.pad_token is None:\n",
        "#                 tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "#             # Benchmark\n",
        "#             result = benchmark_language_model(\n",
        "#                 model=model,\n",
        "#                 tokenizer=tokenizer,\n",
        "#                 prompts=prompts,\n",
        "#                 temperature=temperature,\n",
        "#                 max_new_tokens=max_new_tokens\n",
        "#             )\n",
        "#             all_results[model_name] = result\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"‚ö†Ô∏è Failed to benchmark {model_name}: {e}\")\n",
        "#             all_results[model_name] = {\"error\": str(e)}\n",
        "\n",
        "#         # Clean up to prevent OOM\n",
        "#         del model\n",
        "#         del tokenizer\n",
        "#         gc.collect()\n",
        "#         torch.cuda.empty_cache()\n",
        "#         torch.cuda.ipc_collect()\n",
        "\n",
        "#     return all_results\n",
        "\n",
        "\n",
        "\n",
        "# def print_comparison_table(results):\n",
        "#     print(\"\\nüìä Benchmark Comparison Table:\")\n",
        "#     headers = [\n",
        "#         \"Model\",\n",
        "#         \"P90 TTFT (s)\",\n",
        "#         \"P90 TPS\",\n",
        "#         \"P90 Decode TPS\",\n",
        "#         \"Max GPU Mem (MB)\",\n",
        "#         \"P90 GPU Util (%)\"\n",
        "#     ]\n",
        "#     print(f\"{headers[0]:<45} {headers[1]:<15} {headers[2]:<10} {headers[3]:<15} {headers[4]:<18} {headers[5]}\")\n",
        "#     print(\"-\" * 120)\n",
        "\n",
        "#     for model_name, metrics in results.items():\n",
        "#         print(f\"{model_name:<45} \"\n",
        "#               f\"{metrics['p90_ttft_seconds']:<15.4f} \"\n",
        "#               f\"{metrics['p90_total_tps']:<10.2f} \"\n",
        "#               f\"{metrics['p90_output_decode_tps']:<15.2f} \"\n",
        "#               f\"{metrics['max_gpu_memory_mb']:<18.2f} \"\n",
        "#               f\"{metrics['p90_gpu_utilization']:<.2f}\")\n",
        "\n",
        "\n",
        "# # ===========================\n",
        "# # üß™ Example Usage\n",
        "# # ===========================\n",
        "# model_names = [\n",
        "#     \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "#     \"tiiuae/falcon-rw-1b\"\n",
        "# ]\n",
        "\n",
        "# test_prompts = [\n",
        "#     \"Short prompt.\",\n",
        "#     \"This is a medium length prompt with some context and words.\",\n",
        "#     \"This is a very long prompt designed to simulate a large context. \" * 10\n",
        "# ]\n",
        "\n",
        "# results = compare_llms(\n",
        "#     model_names=model_names,\n",
        "#     prompts=test_prompts,\n",
        "#     temperature=0.7,\n",
        "#     max_new_tokens=100\n",
        "# )\n",
        "\n",
        "# print_comparison_table(results)\n"
      ],
      "metadata": {
        "id": "chI-wg2SV7K3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile compare_llms.py\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlDeviceGetUtilizationRates, nvmlShutdown\n",
        "import torch\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np\n",
        "import threading\n",
        "\n",
        "class GPUMonitor:\n",
        "    def __init__(self, monitoring_interval: float = 0.1, device_index: int = 0):\n",
        "        self.monitoring_interval = monitoring_interval\n",
        "        self.device_index = device_index\n",
        "        self._gpu_memory_usage = []\n",
        "        self._gpu_utilization = []\n",
        "        self._is_monitoring = False\n",
        "        self._thread = None\n",
        "\n",
        "    def _monitor(self):\n",
        "        nvmlInit()\n",
        "        handle = nvmlDeviceGetHandleByIndex(self.device_index)\n",
        "        while self._is_monitoring:\n",
        "            mem_info = nvmlDeviceGetMemoryInfo(handle)\n",
        "            util_info = nvmlDeviceGetUtilizationRates(handle)\n",
        "            self._gpu_memory_usage.append(mem_info.used / (1024 ** 2))  # in MB\n",
        "            self._gpu_utilization.append(util_info.gpu)\n",
        "            time.sleep(self.monitoring_interval)\n",
        "        nvmlShutdown()\n",
        "\n",
        "    def start(self):\n",
        "        self._is_monitoring = True\n",
        "        self._thread = threading.Thread(target=self._monitor)\n",
        "        self._thread.start()\n",
        "\n",
        "    def stop(self):\n",
        "        self._is_monitoring = False\n",
        "        self._thread.join()\n",
        "\n",
        "    def get_peak_usage(self):\n",
        "        return {\n",
        "            'peak_gpu_memory_mb': max(self._gpu_memory_usage, default=0),\n",
        "            'p90_gpu_utilization': np.percentile(self._gpu_utilization, 90) if self._gpu_utilization else 0\n",
        "        }\n",
        "\n",
        "def benchmark_language_model(model, tokenizer, prompts, temperature=0.7, max_new_tokens=100):\n",
        "    model.eval()\n",
        "    model.cuda()\n",
        "\n",
        "    gpu_monitor = GPUMonitor()\n",
        "    gpu_monitor.start()\n",
        "\n",
        "    ttft_list = []\n",
        "    tps_list = []\n",
        "    decode_tps_list = []\n",
        "\n",
        "    for prompt in prompts:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "        input_len = inputs.input_ids.shape[-1]\n",
        "\n",
        "        # Measure time to first token\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Jalankan generate dan ukur waktu hingga token pertama (berdasarkan waktu nyata)\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                **inputs,\n",
        "                do_sample=True,\n",
        "                temperature=temperature,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=False  # tidak perlu lagi\n",
        "            )\n",
        "\n",
        "        gen_time = time.time() - start_time\n",
        "        ttft = gen_time / max(output.sequences.shape[-1] - input_len, 1)\n",
        "\n",
        "\n",
        "        total_tokens = output.sequences.shape[-1]\n",
        "        output_tokens = total_tokens - input_len\n",
        "\n",
        "        ttft_list.append(ttft)\n",
        "        tps_list.append(total_tokens / gen_time)\n",
        "        decode_tps_list.append(output_tokens / gen_time)\n",
        "\n",
        "    gpu_monitor.stop()\n",
        "    gpu_stats = gpu_monitor.get_peak_usage()\n",
        "\n",
        "    return {\n",
        "        'p90_ttft_seconds': np.percentile(ttft_list, 90),\n",
        "        'p90_total_tps': np.percentile(tps_list, 90),\n",
        "        'p90_output_decode_tps': np.percentile(decode_tps_list, 90),\n",
        "        'max_gpu_memory_mb': gpu_stats['peak_gpu_memory_mb'],\n",
        "        'p90_gpu_utilization': gpu_stats['p90_gpu_utilization'],\n",
        "    }\n",
        "\n",
        "def compare_llms(model_names, prompts, temperature=0.7, max_new_tokens=100):\n",
        "    all_results = {}\n",
        "\n",
        "    for model_name in model_names:\n",
        "        print(f\"\\nüîç Benchmarking model: {model_name}\")\n",
        "\n",
        "        try:\n",
        "            # Load model dan tokenizer\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "            ).cuda()\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "            # Setup tokenizer defaults if missing\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "            # Benchmark\n",
        "            result = benchmark_language_model(\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                prompts=prompts,\n",
        "                temperature=temperature,\n",
        "                max_new_tokens=max_new_tokens\n",
        "            )\n",
        "            all_results[model_name] = result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed to benchmark {model_name}: {e}\")\n",
        "            all_results[model_name] = {\"error\": str(e)}\n",
        "\n",
        "        # Clean up to prevent OOM\n",
        "        del model\n",
        "        del tokenizer\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def print_comparison_table(results):\n",
        "    print(\"\\nüìä Benchmark Comparison Table:\")\n",
        "    headers = [\n",
        "        \"Model\",\n",
        "        \"P90 TTFT (s)\",\n",
        "        \"P90 TPS\",\n",
        "        \"P90 Decode TPS\",\n",
        "        \"Max GPU Mem (MB)\",\n",
        "        \"P90 GPU Util (%)\"\n",
        "    ]\n",
        "    print(f\"{headers[0]:<45} {headers[1]:<15} {headers[2]:<10} {headers[3]:<15} {headers[4]:<18} {headers[5]}\")\n",
        "    print(\"-\" * 120)\n",
        "\n",
        "    for model_name, metrics in results.items():\n",
        "        print(f\"{model_name:<45} \"\n",
        "              f\"{metrics['p90_ttft_seconds']:<15.4f} \"\n",
        "              f\"{metrics['p90_total_tps']:<10.2f} \"\n",
        "              f\"{metrics['p90_output_decode_tps']:<15.2f} \"\n",
        "              f\"{metrics['max_gpu_memory_mb']:<18.2f} \"\n",
        "              f\"{metrics['p90_gpu_utilization']:<.2f}\")\n"
      ],
      "metadata": {
        "id": "e_mlTu0xa9E-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180f4344-0826-412b-f8bd-ca8b384ac600"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing compare_llms.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok streamlit"
      ],
      "metadata": {
        "id": "rpLI70_qb1D9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67a9a5ad-1485-4315-9581-fce7fc3bfb15"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.46.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.46.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading pyngrok-7.2.12-py3-none-any.whl (26 kB)\n",
            "Downloading streamlit-1.46.1-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pyngrok, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 pyngrok-7.2.12 streamlit-1.46.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from compare_llms import compare_llms, print_comparison_table\n",
        "import pandas as pd\n",
        "\n",
        "st.set_page_config(page_title=\"LLM Benchmark Comparison\", layout=\"wide\")\n",
        "st.title(\"üîç Benchmark & Compare Language Models (LLMs)\")\n",
        "st.markdown(\"Benchmark LLMs like `DeepSeek`, `Mistral`, `Falcon`, etc. with custom prompts.\")\n",
        "\n",
        "# ===== Step 1: Model Selection =====\n",
        "st.header(\"1. Choose Models to Benchmark\")\n",
        "default_models = [\n",
        "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "    \"tiiuae/falcon-rw-1b\",\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "]\n",
        "models_input = st.text_area(\"Enter model names (one per line)\", \"\\n\".join(default_models))\n",
        "model_names = [m.strip() for m in models_input.splitlines() if m.strip()]\n",
        "\n",
        "# ===== Step 2: Prompt Input =====\n",
        "st.header(\"2. Enter Prompts (You can add multiple)\")\n",
        "prompt_container = st.container()\n",
        "prompts = []\n",
        "\n",
        "# Session state for dynamic prompt inputs\n",
        "if \"prompt_count\" not in st.session_state:\n",
        "    st.session_state.prompt_count = 1\n",
        "\n",
        "for i in range(st.session_state.prompt_count):\n",
        "    prompt = prompt_container.text_input(f\"Prompt {i+1}\", key=f\"prompt_{i}\")\n",
        "    if prompt:\n",
        "        prompts.append(prompt)\n",
        "\n",
        "# Button to add more prompt inputs\n",
        "if prompt_container.button(\"‚ûï Add another prompt\"):\n",
        "    st.session_state.prompt_count += 1\n",
        "\n",
        "# ===== Step 3: Benchmark Settings =====\n",
        "st.header(\"3. Benchmark Settings\")\n",
        "temperature = st.slider(\"Temperature\", 0.0, 1.0, 0.7, 0.1)\n",
        "max_new_tokens = st.slider(\"Max New Tokens\", 10, 200, 100, 10)\n",
        "\n",
        "# ===== Step 4: Run Benchmark =====\n",
        "st.header(\"4. Run Benchmark\")\n",
        "if st.button(\"üöÄ Run Benchmark\"):\n",
        "    if not model_names:\n",
        "        st.warning(\"Please enter at least one model.\")\n",
        "    elif not prompts:\n",
        "        st.warning(\"Please enter at least one prompt.\")\n",
        "    else:\n",
        "        with st.spinner(\"Running benchmark... this may take several minutes...\"):\n",
        "            results = compare_llms(\n",
        "                model_names=model_names,\n",
        "                prompts=prompts,\n",
        "                temperature=temperature,\n",
        "                max_new_tokens=max_new_tokens\n",
        "            )\n",
        "\n",
        "        # Show results table\n",
        "        st.success(\"‚úÖ Benchmark complete!\")\n",
        "        st.subheader(\"üìä Results Table\")\n",
        "\n",
        "        rows = []\n",
        "        for model_name, metrics in results.items():\n",
        "            if \"error\" in metrics:\n",
        "                rows.append({\n",
        "                    \"Model\": model_name,\n",
        "                    \"P90 TTFT (s)\": \"ERROR\",\n",
        "                    \"P90 TPS\": \"ERROR\",\n",
        "                    \"P90 Decode TPS\": \"ERROR\",\n",
        "                    \"Max GPU Mem (MB)\": \"ERROR\",\n",
        "                    \"P90 GPU Util (%)\": \"ERROR\"\n",
        "                })\n",
        "            else:\n",
        "                rows.append({\n",
        "                    \"Model\": model_name,\n",
        "                    \"P90 TTFT (s)\": round(metrics['p90_ttft_seconds'], 4),\n",
        "                    \"P90 TPS\": round(metrics['p90_total_tps'], 2),\n",
        "                    \"P90 Decode TPS\": round(metrics['p90_output_decode_tps'], 2),\n",
        "                    \"Max GPU Mem (MB)\": round(metrics['max_gpu_memory_mb'], 2),\n",
        "                    \"P90 GPU Util (%)\": round(metrics['p90_gpu_utilization'], 2),\n",
        "                })\n",
        "\n",
        "        df = pd.DataFrame(rows)\n",
        "        st.dataframe(df, use_container_width=True)"
      ],
      "metadata": {
        "id": "I1yFrbO1bi68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34023ea4-223c-43c7-b21e-96b0f3bcdfda"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "import threading\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"YOUR_AUTH_TOKEN\" #@param {type:\"string\"}\n",
        "\n",
        "!ngrok authtoken $NGROK_AUTH_TOKEN\n",
        "\n",
        "# Set streamlit port\n",
        "port = 8501\n",
        "\n",
        "# Hentikan dulu ngrok lama jika ada\n",
        "ngrok.kill()\n",
        "\n",
        "# Share link Ngrok\n",
        "public_url = ngrok.connect(port)\n",
        "print(f\"üîó Public URL: {public_url}\")\n",
        "\n",
        "# Jalankan Streamlit di thread terpisah agar tidak blocking\n",
        "def run_streamlit():\n",
        "    os.system(f\"streamlit run app.py --server.port {port}\")\n",
        "\n",
        "thread = threading.Thread(target=run_streamlit)\n",
        "thread.start()\n"
      ],
      "metadata": {
        "id": "qK61Nf7pbuU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b707526e-34bb-4678-9e70-852d82ab2e6a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "üîó Public URL: NgrokTunnel: \"https://be3e3eafaa2d.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}